{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b8e4bf",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf490b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10628dc2",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5944b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           im_name  label\n",
      "0      000231c.jpg      2\n",
      "1      0002574.jpg      5\n",
      "2      00027d5.jpg      7\n",
      "3      000304e.jpg      3\n",
      "4      00047fc.jpg      7\n",
      "...            ...    ...\n",
      "49995  d507197.jpg      5\n",
      "49996  d508429.jpg      1\n",
      "49997  d508cb7.jpg      9\n",
      "49998  d509167.jpg      3\n",
      "49999  d509c42.jpg      3\n",
      "\n",
      "[50000 rows x 2 columns]\n",
      "Original data size: 50000 50000\n",
      "Test data size: 10000 10000\n",
      "(100000, 3072) (100000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import ImageEnhance\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def random_crop(image, crop_size):\n",
    "    \"\"\"\n",
    "    Randomly crop the image to the specified size.\n",
    "\n",
    "    Parameters:\n",
    "        image (PIL.Image): Input image.\n",
    "        crop_size (int): Size of the cropped image.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Cropped image.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    left = random.randint(0, width - crop_size)\n",
    "    upper = random.randint(0, height - crop_size)\n",
    "    right = left + crop_size\n",
    "    lower = upper + crop_size\n",
    "    return image.crop((left, upper, right, lower))\n",
    "\n",
    "def random_rotation(image, max_angle=30):\n",
    "    \"\"\"\n",
    "    Randomly rotate the image within a specified angle range.\n",
    "\n",
    "    Parameters:\n",
    "        image (PIL.Image): Input image.\n",
    "        max_angle (int): Maximum rotation angle in degrees.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Rotated image.\n",
    "    \"\"\"\n",
    "    angle = random.randint(-max_angle, max_angle)\n",
    "    return image.rotate(angle)\n",
    "\n",
    "def color_jitter(image, brightness=0.2, contrast=0.2, color=0.2):\n",
    "    \"\"\"\n",
    "    Apply random color jittering to the image.\n",
    "\n",
    "    Parameters:\n",
    "        image (PIL.Image): Input image.\n",
    "        brightness (float): Brightness adjustment factor range (e.g., 0.2 means [0.8, 1.2]).\n",
    "        contrast (float): Contrast adjustment factor range.\n",
    "        color (float): Color adjustment factor range.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Color-jittered image.\n",
    "    \"\"\"\n",
    "    # Adjust brightness\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    factor = random.uniform(1 - brightness, 1 + brightness)\n",
    "    image = enhancer.enhance(factor)\n",
    "\n",
    "    # Adjust contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    factor = random.uniform(1 - contrast, 1 + contrast)\n",
    "    image = enhancer.enhance(factor)\n",
    "\n",
    "    # Adjust color\n",
    "    enhancer = ImageEnhance.Color(image)\n",
    "    factor = random.uniform(1 - color, 1 + color)\n",
    "    image = enhancer.enhance(factor)\n",
    "\n",
    "    return image\n",
    "\n",
    "def aug_data(X, y, seed=None):\n",
    "    \"\"\"\n",
    "    Augment the data by applying random cropping, random rotation, horizontal flipping, and color jittering.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # Reshape to original image shape (H, W, C)\n",
    "        img = X[i].reshape(3, 32, 32).transpose(1, 2, 0)  # (32, 32, 3)\n",
    "        label = y[i]\n",
    "\n",
    "        # Convert to PIL Image for easier augmentation\n",
    "        pil_img = Image.fromarray(img)\n",
    "\n",
    "        # Append original image\n",
    "        X_aug.append(np.array(pil_img).flatten())\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # Apply augmentations\n",
    "        # Horizontal flip\n",
    "        flipped_img = pil_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        X_aug.append(np.array(flipped_img).flatten())\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # # Random crop\n",
    "        # if random.random() < 0.1:\n",
    "        #     crop_size = random.randint(24, 32)  # Random crop size between 24x24 and 32x32\n",
    "        #     cropped_img = random_crop(pil_img, crop_size).resize((32, 32))\n",
    "        #     X_aug.append(np.array(cropped_img).flatten())\n",
    "        #     y_aug.append(label)\n",
    "\n",
    "        # # Random rotation\n",
    "        # if random.random() < 0.1:\n",
    "        #     rotated_img = random_rotation(pil_img).resize((32, 32))\n",
    "        #     X_aug.append(np.array(rotated_img).flatten())\n",
    "        #     y_aug.append(label)\n",
    "\n",
    "        # # Color jittering\n",
    "        # if random.random() < 0.1:\n",
    "        #     jittered_img = color_jitter(pil_img)\n",
    "        #     X_aug.append(np.array(jittered_img).flatten())\n",
    "        #     y_aug.append(label)\n",
    "\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "\n",
    "csv_path, csv_test_path = \"../../data/train.csv\", \"../../data/test.csv\"\n",
    "img_dir, img_dir_test = \"../../data/train_ims\", \"../../data/test_ims\"\n",
    "\n",
    "data_train = pd.read_csv(csv_path)\n",
    "print(data_train)\n",
    "data_test = pd.read_csv(csv_test_path)\n",
    "\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "for _, row in data_train.iterrows():\n",
    "    img_path = os.path.join(img_dir, row.iloc[0])\n",
    "    label = int(row.iloc[1])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = np.array(img).flatten()\n",
    "    X_train.append(img)\n",
    "    y_train.append(label)\n",
    "\n",
    "for _, row in data_test.iterrows():\n",
    "    img_path = os.path.join(img_dir_test, row.iloc[0])\n",
    "    label = int(row.iloc[1])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = np.array(img).flatten()\n",
    "    X_test.append(img)\n",
    "    y_test.append(label)\n",
    "# Example usage\n",
    "print(\"Original data size:\", len(X_train), len(y_train))\n",
    "print(\"Test data size:\", len(X_test), len(y_test))\n",
    "# Augment the training data\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_train, y_train = aug_data(X_train, y_train, SEED)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dca99c",
   "metadata": {},
   "source": [
    "### Size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "941cba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set (after flatten 3 * 32 * 32):  (100000, 3072)\n",
      "The shape of the testing set (after flatten 3 * 32 * 32):  (10000, 3072)\n",
      "The size of the label of the training set:  (100000,)\n",
      "The size of the label of the test set:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the training set (after flatten 3 * 32 * 32): \", X_train.shape)\n",
    "print(\"The shape of the testing set (after flatten 3 * 32 * 32): \", X_test.shape)\n",
    "print(\"The size of the label of the training set: \", y_train.shape)\n",
    "print(\"The size of the label of the test set: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f85c8",
   "metadata": {},
   "source": [
    "### Calculate the number of the labels of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b48f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each label:\n",
      "       Count\n",
      "Label       \n",
      "0      10076\n",
      "1      10032\n",
      "2      10064\n",
      "3       9982\n",
      "4       9964\n",
      "5       9934\n",
      "6       9970\n",
      "7       9996\n",
      "8      10004\n",
      "9       9978\n"
     ]
    }
   ],
   "source": [
    "# Count the labels\n",
    "count_labels = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Create DataFrame\n",
    "label_counts_df = pd.DataFrame({\n",
    "    \"Label\": count_labels[0],\n",
    "    \"Count\": count_labels[1]\n",
    "}).set_index(\"Label\")\n",
    "\n",
    "# Print DataFrame\n",
    "print(\"Count of each label:\")\n",
    "print(label_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532638d",
   "metadata": {},
   "source": [
    "### Draw Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f69e7e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other Statistics for the training set:\n",
      "count    100000.000000\n",
      "mean          4.492580\n",
      "std           2.875376\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           4.000000\n",
      "75%           7.000000\n",
      "max           9.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Other Statistics for the training set:\")\n",
    "print(pd.Series(y_train).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf0ea9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import random\n",
    "\n",
    "# def random_crop(image, crop_size):\n",
    "#     \"\"\"\n",
    "#     Randomly crop the image to the specified size.\n",
    "\n",
    "#     Parameters:\n",
    "#         image (PIL.Image): Input image.\n",
    "#         crop_size (int): Size of the cropped image.\n",
    "\n",
    "#     Returns:\n",
    "#         PIL.Image: Cropped image.\n",
    "#     \"\"\"\n",
    "#     width, height = image.size\n",
    "#     left = random.randint(0, width - crop_size)\n",
    "#     upper = random.randint(0, height - crop_size)\n",
    "#     right = left + crop_size\n",
    "#     lower = upper + crop_size\n",
    "#     return image.crop((left, upper, right, lower))\n",
    "\n",
    "# def random_rotation(image, max_angle=30):\n",
    "#     \"\"\"\n",
    "#     Randomly rotate the image within a specified angle range.\n",
    "\n",
    "#     Parameters:\n",
    "#         image (PIL.Image): Input image.\n",
    "#         max_angle (int): Maximum rotation angle in degrees.\n",
    "\n",
    "#     Returns:\n",
    "#         PIL.Image: Rotated image.\n",
    "#     \"\"\"\n",
    "#     angle = random.randint(-max_angle, max_angle)\n",
    "#     return image.rotate(angle)\n",
    "\n",
    "# def color_jitter(image, brightness=0.2, contrast=0.2, color=0.2):\n",
    "#     \"\"\"\n",
    "#     Apply random color jittering to the image.\n",
    "\n",
    "#     Parameters:\n",
    "#         image (PIL.Image): Input image.\n",
    "#         brightness (float): Brightness adjustment factor range (e.g., 0.2 means [0.8, 1.2]).\n",
    "#         contrast (float): Contrast adjustment factor range.\n",
    "#         color (float): Color adjustment factor range.\n",
    "\n",
    "#     Returns:\n",
    "#         PIL.Image: Color-jittered image.\n",
    "#     \"\"\"\n",
    "#     # Adjust brightness\n",
    "#     enhancer = ImageEnhance.Brightness(image)\n",
    "#     factor = random.uniform(1 - brightness, 1 + brightness)\n",
    "#     image = enhancer.enhance(factor)\n",
    "\n",
    "#     # Adjust contrast\n",
    "#     enhancer = ImageEnhance.Contrast(image)\n",
    "#     factor = random.uniform(1 - contrast, 1 + contrast)\n",
    "#     image = enhancer.enhance(factor)\n",
    "\n",
    "#     # Adjust color\n",
    "#     enhancer = ImageEnhance.Color(image)\n",
    "#     factor = random.uniform(1 - color, 1 + color)\n",
    "#     image = enhancer.enhance(factor)\n",
    "\n",
    "#     return image\n",
    "\n",
    "# def aug_data(X, y, seed=None):\n",
    "#     \"\"\"\n",
    "#     Augment the data by applying random cropping, random rotation, horizontal flipping, and color jittering.\n",
    "#     \"\"\"\n",
    "#     if seed is not None:\n",
    "#         random.seed(seed)\n",
    "#         np.random.seed(seed)\n",
    "\n",
    "#     X_aug = []\n",
    "#     y_aug = []\n",
    "\n",
    "#     for i in range(len(X)):\n",
    "#         # Reshape to original image shape (H, W, C)\n",
    "#         img = X[i].reshape(3, 32, 32).transpose(1, 2, 0)  # (32, 32, 3)\n",
    "#         label = y[i]\n",
    "\n",
    "#         # Convert to PIL Image for easier augmentation\n",
    "#         pil_img = Image.fromarray(img)\n",
    "\n",
    "#         # Append original image\n",
    "#         X_aug.append(np.array(pil_img).flatten())\n",
    "#         y_aug.append(label)\n",
    "\n",
    "#         # Apply augmentations\n",
    "#         # Horizontal flip\n",
    "#         flipped_img = pil_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#         X_aug.append(np.array(flipped_img).flatten())\n",
    "#         y_aug.append(label)\n",
    "\n",
    "#         # Random crop\n",
    "#         if random.random() < 0.1:\n",
    "#             crop_size = random.randint(24, 32)  # Random crop size between 24x24 and 32x32\n",
    "#             cropped_img = random_crop(pil_img, crop_size).resize((32, 32))\n",
    "#             X_aug.append(np.array(cropped_img).flatten())\n",
    "#             y_aug.append(label)\n",
    "\n",
    "#         # Random rotation\n",
    "#         if random.random() < 0.1:\n",
    "#             rotated_img = random_rotation(pil_img).resize((32, 32))\n",
    "#             X_aug.append(np.array(rotated_img).flatten())\n",
    "#             y_aug.append(label)\n",
    "\n",
    "#         # Color jittering\n",
    "#         if random.random() < 0.1:\n",
    "#             jittered_img = color_jitter(pil_img)\n",
    "#             X_aug.append(np.array(jittered_img).flatten())\n",
    "#             y_aug.append(label)\n",
    "\n",
    "#     return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "# # Example usage\n",
    "# X_train_aug, y_train_aug = aug_data(X_train, y_train, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data after augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0bb72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Augmented training set shape: \", X_train_aug.shape)\n",
    "# print(\"Augmented training set labels shape: \", y_train_aug.shape)\n",
    "# # Count the labels\n",
    "# count_labels = np.unique(y_train_aug, return_counts=True)\n",
    "# # Create DataFrame\n",
    "# label_counts_df = pd.DataFrame({\n",
    "#     \"Label\": count_labels[0],\n",
    "#     \"Count\": count_labels[1]\n",
    "# }).set_index(\"Label\")\n",
    "# # Print DataFrame\n",
    "# print(\"Count of each label in augmented training set:\")\n",
    "# print(label_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd55cda",
   "metadata": {},
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d38c25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# def resize_images(images, target_size=(128, 128)):\n",
    "#     \"\"\"\n",
    "#     Resize a list or array of images to the target size.\n",
    "\n",
    "#     Parameters:\n",
    "#         images (numpy.ndarray): Input images, shape (N, H, W, C) or (N, H*W*C).\n",
    "#         target_size (tuple): Target size (width, height), default is (128, 128).\n",
    "\n",
    "#     Returns:\n",
    "#         numpy.ndarray: Resized images, shape (N, target_size[1], target_size[0], C).\n",
    "#     \"\"\"\n",
    "#     resized_images = []\n",
    "#     for img in images:\n",
    "#         # If the image is flattened, reshape it to (H, W, C)\n",
    "#         if len(img.shape) == 1:\n",
    "#             img = img.reshape(32, 32, 3)  # Adjust this based on your original image shape\n",
    "\n",
    "#         # Resize the image\n",
    "#         resized_img = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "#         resized_images.append(resized_img)\n",
    "\n",
    "#     return np.array(resized_images)\n",
    "\n",
    "# X_train_aug_resized = resize_images(X_train_aug, target_size=(128, 128))\n",
    "# print(\"Resized images shape:\", X_train_aug_resized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5c53b",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff1a83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (80000, 3072)\n",
      "Testing set shape: (20000, 3072)\n",
      "Training labels shape: (80000,)\n",
      "Testing labels shape: (20000,)\n",
      "Flattened training set shape: (80000, 3072)\n",
      "Flattened testing set shape: (20000, 3072)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_val.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Testing labels shape:\", y_val.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  # Flatten the images\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)  # Flatten the images\n",
    "print(\"Flattened training set shape:\", X_train.shape)\n",
    "print(\"Flattened testing set shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7128b6c",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07711f1d",
   "metadata": {},
   "source": [
    "### Extract original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ceacca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_statistics_extractor(image):\n",
    "    \"\"\"\n",
    "    Extract statistical features from the original image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Statistical feature vector for the input image.\n",
    "    \"\"\"\n",
    "    # If the image is flattened, reshape it to (H, W, C)\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3)  # Adjust this based on your original image shape\n",
    "\n",
    "    resized_image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return resized_image.flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58825fbc",
   "metadata": {},
   "source": [
    "### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41e3c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def SIFT_extractor(image, n_features=128):\n",
    "    \"\"\"\n",
    "    Extract SIFT features from a single image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: SIFT descriptors for the input image. If no descriptors are found, returns an empty array.\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        image = np.array(image)\n",
    "\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3) \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    # If no descriptors are found, return an empty array\n",
    "    if descriptors is None:\n",
    "        return np.zeros((1, n_features)).flatten()\n",
    "\n",
    "    return np.mean(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e222f0",
   "metadata": {},
   "source": [
    "### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35bf03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def HOG_extractor(image, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extract HOG features from a single image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "        target_size (tuple): Target size to resize the image before extracting HOG features.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Flattened HOG feature vector for the input image.\n",
    "    \"\"\"\n",
    "    # If the image is flattened, reshape it to (H, W, C)\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3)  # Adjust this based on your original image shape\n",
    "    \n",
    "    # Resize the image to the target size\n",
    "    resized_img = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # # Convert to grayscale (HOG works on single-channel images)\n",
    "    gray_img = cv2.cvtColor(resized_img, cv2.COLOR_RGB2GRAY)\n",
    "    # print(\"Gray image shape:\", gray_img.shape)\n",
    "\n",
    "\n",
    "    # Split the image into three channels (R, G, B)\n",
    "    channels = cv2.split(resized_img)\n",
    "    # print(\"Channels shape:\", [channel.shape for channel in channels])\n",
    "\n",
    "    # Compute HOG features\n",
    "    hog_kwargs = {\n",
    "        \"_winSize\": (128, 128),\n",
    "        \"_blockSize\": (64, 64),\n",
    "        \"_blockStride\": (16, 16),\n",
    "        \"_cellSize\": (16, 16),\n",
    "        \"_nbins\": 10,\n",
    "        \"_derivAperture\": 1,\n",
    "        \"_winSigma\": -1,\n",
    "        \"_histogramNormType\": 0,\n",
    "        \"_L2HysThreshold\": 0.2,\n",
    "        \"_gammaCorrection\": True,\n",
    "        \"_nlevels\": 64,\n",
    "        \"_signedGradient\": True\n",
    "        \n",
    "    }   \n",
    "\n",
    "    hog = cv2.HOGDescriptor(**hog_kwargs)\n",
    "\n",
    "    # Compute HOG features\n",
    "    hog_features = []\n",
    "    for channel in channels:\n",
    "        # Compute HOG features for the current channel\n",
    "        hog_feature = hog.compute(channel)\n",
    "        # print (\"HOG feature shape:\", hog_feature.shape)\n",
    "        hog_features.append(hog_feature.flatten())\n",
    "    \n",
    "    # gray_hog_feature = hog.compute(gray_img)\n",
    "    # print (\"Gray HOG feature shape:\", gray_hog_feature.shape)\n",
    "\n",
    "    # Flatten the HOG feature vector\n",
    "    return np.concatenate(hog_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711c201",
   "metadata": {},
   "source": [
    "### EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdf83933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EOG_extractor(image, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Extract Edge Orientation Gradient (EOG) features from a single image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "        target_size (tuple): Target size to resize the image before extracting EOG features.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Flattened EOG feature vector for the input image.\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3)\n",
    "    # Resize the image to the target size\n",
    "    resized_img = cv2.resize(image, target_size, interpolation=cv2.INTER_CUBIC)\n",
    "    # Convert to grayscale\n",
    "    gray_img = cv2.cvtColor(resized_img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray_img, threshold1=100, threshold2=200)\n",
    "    # Compute gradients in x and y directions\n",
    "    grad_x = cv2.Sobel(edges, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(edges, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    # Compute gradient magnitude and orientation\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    orientation = np.arctan2(grad_y, grad_x)\n",
    "    # Flatten the magnitude and orientation into a feature vector\n",
    "    eog_feature = np.concatenate([magnitude.flatten(), orientation.flatten()])\n",
    "\n",
    "    return eog_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a30f3",
   "metadata": {},
   "source": [
    "### LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "936e7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "def LBP_extractor(image, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extract Local Binary Pattern (LBP) features from a single image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "        target_size (tuple): Target size to resize the image before extracting LBP features.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Flattened LBP feature vector for the input image.\n",
    "    \"\"\"\n",
    "    # If the image is flattened, reshape it to (H, W, C)\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3)  # Adjust this based on your original image shape\n",
    "    # Resize the image to the target size\n",
    "    resized_img = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    # Convert to grayscale\n",
    "    gray_img = rgb2gray(resized_img)\n",
    "    # Compute LBP features\n",
    "    lbp = local_binary_pattern(gray_img, P=8, R=1, method='uniform')\n",
    "    # Flatten the LBP feature vector\n",
    "    return lbp.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953e436",
   "metadata": {},
   "source": [
    "### ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b30f71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ORB_extractor(image, target_size=(64, 64), n_features=500, fixed_length=2000):\n",
    "    \"\"\"\n",
    "    Extract ORB features from a single image with fixed length.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "        target_size (tuple): Target size to resize the image before extracting ORB features.\n",
    "        n_features (int): Maximum number of features to retain.\n",
    "        fixed_length (int): Fixed length for the ORB feature vector.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Flattened ORB descriptors with fixed length. If no descriptors are found, returns a zero vector.\n",
    "    \"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=n_features)\n",
    "\n",
    "    # If the image is flattened, reshape it to (H, W, C)\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3)\n",
    "\n",
    "    # Resize the image to the target size\n",
    "    resized_img = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    # Convert to grayscale\n",
    "    gray_img = cv2.cvtColor(resized_img, cv2.COLOR_RGB2GRAY)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray_img, None)\n",
    "    # If no descriptors are found, return a zero vector\n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((1, 32), dtype=np.uint8)\n",
    "    # Flatten the descriptors\n",
    "    descriptors = descriptors.flatten()\n",
    "\n",
    "    # Ensure the feature vector has a fixed length\n",
    "    if len(descriptors) < fixed_length:\n",
    "        descriptors = np.pad(descriptors, (0, fixed_length - len(descriptors)), mode='constant')\n",
    "    elif len(descriptors) > fixed_length:\n",
    "        descriptors = descriptors[:fixed_length]\n",
    "\n",
    "    return descriptors.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0e7fd",
   "metadata": {},
   "source": [
    "### HIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "021e6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HIST_extractor(image, target_size=(64, 64), bins=(8, 8, 8)):\n",
    "    \"\"\"\n",
    "    Extract color histogram (HIST) features from a single image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "        target_size (tuple): Target size to resize the image before extracting HIST features.\n",
    "        bins (tuple): Number of bins for each color channel (e.g., (8, 8, 8)).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Flattened HIST feature vector for the input image.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(image.shape) == 1:\n",
    "        image = image.reshape(32, 32, 3)\n",
    "\n",
    "    # Resize the image to the target size\n",
    "    resized_img = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    # Compute the color histogram for each channel (B, G, R)\n",
    "    hist = cv2.calcHist([resized_img], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    # Normalize the histogram\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316ea8c",
   "metadata": {},
   "source": [
    "### Combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e93fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_flag = False\n",
    "\n",
    "def combined_feature_extractor(image):\n",
    "    \"\"\"\n",
    "    Extract combined features (HOG, EOG, LBP, ORB, HIST, SIFT) from a single input image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, shape (H, W, C) or (H*W*C).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Combined feature vector for the input image.\n",
    "    \"\"\"\n",
    "    # # Extract features\n",
    "    # static_features = image_statistics_extractor(image)  # Original image statistics\n",
    "    # sift_features = SIFT_extractor(image)  # SIFT features\n",
    "    # hog_features = HOG_extractor(image)   # HOG features\n",
    "    # eog_features = EOG_extractor(image)   # EOG features\n",
    "    # # lbp_features = LBP_extractor(image)   # LBP features\n",
    "    # orb_features = ORB_extractor(image)   # ORB features\n",
    "    # hist_features = HIST_extractor(image) # HIST features\n",
    "\n",
    "    global print_flag\n",
    "\n",
    "    # static_features = np.array(image_statistics_extractor(image), dtype=np.float32)\n",
    "    sift_features = np.array(SIFT_extractor(image), dtype=np.float32)\n",
    "    hog_features = np.array(HOG_extractor(image), dtype=np.float32)\n",
    "    eog_features = np.array(EOG_extractor(image), dtype=np.float32)\n",
    "    # lbp_features = np.array(LBP_extractor(image))\n",
    "    orb_features = np.array(ORB_extractor(image), dtype=np.float32)\n",
    "    hist_features = np.array(HIST_extractor(image), dtype=np.float32)\n",
    "\n",
    "    # Combine features into a single vector\n",
    "    combined_features = np.concatenate((\n",
    "        # static_features,\n",
    "        # sift_features,\n",
    "        hog_features,\n",
    "        # eog_features,\n",
    "        # lbp_features,\n",
    "        # orb_features,\n",
    "        # hist_features,\n",
    "    ))\n",
    "\n",
    "    # Print feature shapes for debugging\n",
    "    if print_flag == False:\n",
    "        # print(\"Static features shape:\", static_features.shape)\n",
    "        print(\"HOG features shape:\", hog_features.shape)\n",
    "        print(\"EOG features shape:\", eog_features.shape)\n",
    "        print(\"ORB features shape:\", orb_features.shape)\n",
    "        print(\"HIST features shape:\", hist_features.shape)\n",
    "        print(\"SIFT features shape:\", sift_features.shape)\n",
    "        print_flag = True\n",
    "    \n",
    "    # del static_features\n",
    "    del sift_features\n",
    "    del hog_features\n",
    "    del eog_features\n",
    "    # del lbp_features\n",
    "    del orb_features\n",
    "    del hist_features\n",
    "    \n",
    "    # RECORD MANUALLY THE SHAPE OF EACH FEATURE\n",
    "    # Static features shape: (12288,)\n",
    "    # HOG features shape: (8100,)\n",
    "    # EOG features shape: (8192,)\n",
    "    # ORB features shape: (8000,)\n",
    "    # HIST features shape: (512,)\n",
    "    # SIFT features shape: (128,)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81bd64",
   "metadata": {},
   "source": [
    "### Convert training set with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afe655de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for X_train:   0%|          | 0/80000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG features shape: (12000,)\n",
      "EOG features shape: (2048,)\n",
      "ORB features shape: (2000,)\n",
      "HIST features shape: (512,)\n",
      "SIFT features shape: (128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for X_train: 100%|██████████| 80000/80000 [03:47<00:00, 351.95it/s]\n",
      "Extracting features for X_val: 100%|██████████| 20000/20000 [01:08<00:00, 290.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features from training and validation sets\n",
    "X_train_features = [combined_feature_extractor(img) for img in tqdm(X_train, desc=\"Extracting features for X_train\", total=len(X_train))]\n",
    "X_val_features = [combined_feature_extractor(img) for img in tqdm(X_val, desc=\"Extracting features for X_val\", total=len(X_val))]\n",
    "\n",
    "\n",
    "\n",
    "# # Convert lists to numpy arrays\n",
    "# X_train_featured = np.array(X_train_features)\n",
    "# X_val_featured = np.array(X_val_features)\n",
    "\n",
    "# Print shapes\n",
    "# print(\"X_train_featured shape:\", X_train_features.shape)\n",
    "# print(\"X_val_featured shape:\", X_val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed38cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_featured shape: (80000, 12000)\n",
      "X_val_featured shape: (20000, 12000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_featured = np.array(X_train_features, dtype=np.float32)\n",
    "X_val_featured = np.array(X_val_features, dtype=np.float32)\n",
    "del X_train_features\n",
    "del X_val_features\n",
    "\n",
    "print(\"X_train_featured shape:\", X_train_featured.shape)\n",
    "print(\"X_val_featured shape:\", X_val_featured.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e03ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(X_train_features)):\n",
    "#     print(f\"Image {i} has shape {X_train_features[i].shape}\")\n",
    "# for i in range(len(X_val_features)):\n",
    "#     print(f\"Image {i} has shape {X_val_features[i].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d25d5",
   "metadata": {},
   "source": [
    "### SVM + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaf644",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.15 GiB for an array with shape (80000, 12000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      7\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[0;32m      8\u001b[0m     StandardScaler(), \n\u001b[0;32m      9\u001b[0m     PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED), \n\u001b[0;32m     10\u001b[0m     SVC(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m,random_state\u001b[38;5;241m=\u001b[39mSEED))\n\u001b[1;32m---> 12\u001b[0m svm_model\u001b[38;5;241m.\u001b[39mfit(X_train_featured, y_train)\n\u001b[0;32m     14\u001b[0m y_pred_svm \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(X_val_featured)\n\u001b[0;32m     15\u001b[0m accuracy_svm \u001b[38;5;241m=\u001b[39m accuracy_score(y_val, y_pred_svm)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    407\u001b[0m     cloned_transformer,\n\u001b[0;32m    408\u001b[0m     X,\n\u001b[0;32m    409\u001b[0m     y,\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    412\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    413\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    414\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1314\u001b[0m         )\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:999\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m _incremental_mean_and_var(\n\u001b[0;32m   1000\u001b[0m             X,\n\u001b[0;32m   1001\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_,\n\u001b[0;32m   1002\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_,\n\u001b[0;32m   1003\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_,\n\u001b[0;32m   1004\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1005\u001b[0m         )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1143\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     T \u001b[38;5;241m=\u001b[39m new_sum \u001b[38;5;241m/\u001b[39m new_sample_count\n\u001b[1;32m-> 1143\u001b[0m     temp \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m T\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m         correction \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1148\u001b[0m             np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1149\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.15 GiB for an array with shape (80000, 12000) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    PCA(n_components=0.75, random_state=SEED), \n",
    "    SVC(C=8, kernel='rbf', gamma='scale',random_state=SEED))\n",
    "\n",
    "svm_model.fit(X_train_featured, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_val_featured)\n",
    "accuracy_svm = accuracy_score(y_val, y_pred_svm)\n",
    "print(f\"SVM Validation Accuracy: {accuracy_svm:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# # Confusion Matrix\n",
    "# cm = confusion_matrix(y_val, y_pred_svm)\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_val), yticklabels=np.unique(y_val)) \n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe773c5",
   "metadata": {},
   "source": [
    "### Predict test data's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a939a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# X_test_features = []\n",
    "# X_test_resized = resize_images(X_test)\n",
    "# for image in X_test_resized:\n",
    "#     combined_features = combine_feature(image)\n",
    "#     X_test_features.append(combined_features)\n",
    "# X_test_features = np.array(X_test_features)\n",
    "# print(f\"Preprocessed test images shape: {X_test_features.shape}\")\n",
    "\n",
    "# # Predict labels\n",
    "# y_test_pred = svm_clf.predict(X_test_features)\n",
    "\n",
    "# # Save predictions to CSV\n",
    "# test_df['label'] = y_test_pred\n",
    "# submission_file = './submission.csv'\n",
    "# test_df.to_csv(submission_file, index=False)\n",
    "\n",
    "# print(f\"Predictions saved to {submission_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
